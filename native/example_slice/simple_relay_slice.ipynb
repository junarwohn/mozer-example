{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jd/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"true\"\n",
    "import tvm\n",
    "from tvm import relay\n",
    "from tvm.relay.dataflow_pattern import rewrite\n",
    "from tvm.relay.dataflow_pattern import *\n",
    "import numpy as np\n",
    "from tvm.relay import transform\n",
    "from tvm.relay.testing import run_opt_pass\n",
    "from tvm.relay import transform, build_module\n",
    "from tvm.relay import testing\n",
    "import tvm.testing\n",
    "# from SlicingMachine import TVMSlicer\n",
    "from mozer.slicer.SlicingMachine import TVMSlicer\n",
    "from mozer.slicer.Quantize import quantize\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import tvm\n",
    "import tvm.relay as relay\n",
    "from tvm.contrib import graph_executor \n",
    "import numpy as np\n",
    "import json\n",
    "import pygraphviz as pgv\n",
    "from argparse import ArgumentParser\n",
    "from tvm.relay.build_module import bind_params_by_name\n",
    "from tvm.relay.dataflow_pattern import *\n",
    "from tvm import rpc\n",
    "from tvm.autotvm.measure.measure_methods import set_cuda_target_arch\n",
    "from tensorflow import keras\n",
    "from tvm.contrib.download import download_testdata\n",
    "import subprocess\n",
    "from tvm.contrib import relay_viz\n",
    "from tvm.relay import build_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_split(expr, split_conf, params=None):\n",
    "    \"\"\"Splitting the graph into a list of subgraphs\"\"\"\n",
    "\n",
    "    def get_dep_var(sub_var_dep):\n",
    "        return [var for var in sub_var_dep[len(sub_var_dep) - 1][\"ref_nodes\"]]\n",
    "\n",
    "    def parse_dependency(value, snode_dep, new_input_idx):\n",
    "        new_args = []\n",
    "        need_update = False\n",
    "        for var in value.args:\n",
    "            is_free_var = False\n",
    "            for dep in snode_dep[:-1]:\n",
    "                if var in dep[\"nodes\"]:\n",
    "                    # Mark the previous subgraph node as a dependency.\n",
    "                    dep[\"nodes\"][var] += 1\n",
    "                    dep[\"ref_nodes\"][var] = dep[\"nodes\"][var]\n",
    "                    # The var of this call is a free_var\n",
    "                    is_free_var = True\n",
    "            # if the var of this call is a free_var, recreate it and give it a fixed input name.\n",
    "            # if is_free_var:\n",
    "            #     need_update = True\n",
    "            #     new_args.append(relay.var(f\"data_n_{new_input_idx}\", var.checked_type))\n",
    "            #     new_input_idx += 1\n",
    "            # else:\n",
    "            new_args.append(var)\n",
    "        # if the 'tvm.relay.expr.Call' has a free_var, recreate it with new name as 'data_n_*'.\n",
    "        if need_update:\n",
    "            value = tvm.relay.expr.Call(\n",
    "                value.op, new_args, value.attrs, value.type_args, value.span\n",
    "            )\n",
    "        return value, snode_dep, new_input_idx\n",
    "\n",
    "    def merge_constant_expr(constant_expr, expr):\n",
    "        # merge constant express with a express\n",
    "        if not isinstance(constant_expr.body, tvm.relay.expr.Let):\n",
    "            return tvm.relay.expr.Let(constant_expr.var, constant_expr.value, expr)\n",
    "\n",
    "        return tvm.relay.expr.Let(\n",
    "            constant_expr.var, constant_expr.value, merge_constant_expr(constant_expr.body, expr)\n",
    "        )\n",
    "\n",
    "    def _recursion(anf, pipeline_mods, split_conf, constant_expr):\n",
    "        # Enumurate all operators of compute graph, then split the compute graph into a group of\n",
    "        # subgraph.\n",
    "        nonlocal operator_index_map\n",
    "        nonlocal new_input_idx\n",
    "        nonlocal snode_dep\n",
    "        # Get last element in snode_dep : current node's dependency\n",
    "        cur_node_dep = snode_dep[len(snode_dep) - 1]\n",
    "        # If function -> decouple\n",
    "        if isinstance(anf, tvm.relay.Function):\n",
    "            return tvm.relay.Function(\n",
    "                anf.params,\n",
    "                _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "                anf.ret_type,\n",
    "                anf.type_params,\n",
    "                anf.attrs,\n",
    "            )\n",
    "        # Function of Let\n",
    "        if isinstance(anf, tvm.relay.expr.Let):\n",
    "            value = anf.value\n",
    "            # record the constant expr to make sure all sugraphs can find correct constant.\n",
    "            if isinstance(value, tvm.relay.expr.Constant):\n",
    "                # cosntant_expr is initally None\n",
    "                if not constant_expr:\n",
    "                    constant_expr = tvm.relay.expr.Let(anf.var, value, anf.var)\n",
    "                else:\n",
    "                    constant_expr = tvm.relay.expr.Let(anf.var, value, constant_expr)\n",
    "            if isinstance(value, tvm.relay.expr.Call):\n",
    "                new_args = []\n",
    "                # build current var list\n",
    "                cur_node_dep[\"nodes\"][anf.var] = 0\n",
    "                # Get the dependency information of the nodes.\n",
    "                value, snode_dep, new_input_idx = parse_dependency(value, snode_dep, new_input_idx)\n",
    "                if isinstance(value.op, tvm.ir.Op):\n",
    "                    if value.op.name in operator_index_map:\n",
    "                        operator_index_map[value.op.name] += 1\n",
    "                    else:\n",
    "                        operator_index_map[value.op.name] = 0\n",
    "                    split_operator_name = split_conf[0][\"op_name\"] if split_conf else \"\"\n",
    "                    split_operator_index = split_conf[0][\"op_index\"] if split_conf else \"\"\n",
    "                    # if a operator name and repeating count in the network match with the values\n",
    "                    # of the 'split configuration', then this place is where we should do the\n",
    "                    # graph splitting.\n",
    "                    if (\n",
    "                        split_conf\n",
    "                        and split_operator_name in operator_index_map\n",
    "                        and operator_index_map[split_operator_name] >= split_operator_index\n",
    "                    ):\n",
    "                        # Do graph splitting.\n",
    "                        split_conf.pop(0)\n",
    "                        snode_dep.append({\"nodes\": {}, \"ref_nodes\": {}})\n",
    "                        ann = _recursion(\n",
    "                            anf.body,\n",
    "                            pipeline_mods,\n",
    "                            split_conf,\n",
    "                            constant_expr,\n",
    "                        )\n",
    "                        snode_dep.pop()\n",
    "                        dep_vars = get_dep_var(snode_dep)\n",
    "                        # When the nodes of the current subgraph are the depedency node of another\n",
    "                        # subgraph, we need to set them as the output of current subgraph.\n",
    "                        body = relay.Tuple(dep_vars) if len(dep_vars) > 1 else anf.var\n",
    "                        # when the operator of current subgraph uses previous subgraph constant\n",
    "                        # as the argument of a \"relay.expr.call\", such constant may become a free\n",
    "                        # varaible if the constant does not exist in the current subgraph.\n",
    "                        # merge the previous constant with current subgraph to avoid such issue.\n",
    "                        if constant_expr:\n",
    "                            ann = merge_constant_expr(constant_expr, ann)\n",
    "                        ann = run_opt_pass(ann, transform.ToGraphNormalForm())\n",
    "                        mod = tvm.IRModule.from_expr(ann)\n",
    "                        pipeline_mods.insert(0, mod)\n",
    "                        # Return the last node of the current subgraph.\n",
    "                        return tvm.relay.expr.Let(anf.var, value, body)\n",
    "            return tvm.relay.expr.Let(\n",
    "                anf.var,\n",
    "                value,\n",
    "                _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "            )\n",
    "        # Or End\n",
    "        else:\n",
    "            return anf\n",
    "    \n",
    "    # def getting_inputs(mod):\n",
    "    #     name_hints = []\n",
    "    #     for param in mod['main'].params:\n",
    "    #         name_hints.append(param.name_hint)\n",
    "    #     return name_hints\n",
    "\n",
    "    # def setting_outputs(mod, name_hints):\n",
    "    #     # Get last element in snode_dep : current node's dependency\n",
    "    #     # If function -> decouple\n",
    "    #     if isinstance(anf, tvm.relay.Function):\n",
    "    #         return tvm.relay.Function(\n",
    "    #             anf.params,\n",
    "    #             _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "    #             anf.ret_type,\n",
    "    #             anf.type_params,\n",
    "    #             anf.attrs,\n",
    "    #         )\n",
    "    #     # Function of Let\n",
    "    #     if isinstance(anf, tvm.relay.expr.Let):\n",
    "    #         value = anf.value\n",
    "    #         # record the constant expr to make sure all sugraphs can find correct constant.\n",
    "    #         if isinstance(value, tvm.relay.expr.Constant):\n",
    "    #             # cosntant_expr is initally None\n",
    "    #             if not constant_expr:\n",
    "    #                 constant_expr = tvm.relay.expr.Let(anf.var, value, anf.var)\n",
    "    #             else:\n",
    "    #                 constant_expr = tvm.relay.expr.Let(anf.var, value, constant_expr)\n",
    "    #         if isinstance(value, tvm.relay.expr.Call):\n",
    "    #             new_args = []\n",
    "    #             # build current var list\n",
    "    #             cur_node_dep[\"nodes\"][anf.var] = 0\n",
    "    #             # Get the dependency information of the nodes.\n",
    "    #             value, snode_dep, new_input_idx = parse_dependency(value, snode_dep, new_input_idx)\n",
    "    #             if isinstance(value.op, tvm.ir.Op):\n",
    "    #                 if value.op.name in operator_index_map:\n",
    "    #                     operator_index_map[value.op.name] += 1\n",
    "    #                 else:\n",
    "    #                     operator_index_map[value.op.name] = 0\n",
    "    #                 split_operator_name = split_conf[0][\"op_name\"] if split_conf else \"\"\n",
    "    #                 split_operator_index = split_conf[0][\"op_index\"] if split_conf else \"\"\n",
    "    #                 # if a operator name and repeating count in the network match with the values\n",
    "    #                 # of the 'split configuration', then this place is where we should do the\n",
    "    #                 # graph splitting.\n",
    "    #                 if (\n",
    "    #                     split_conf\n",
    "    #                     and split_operator_name in operator_index_map\n",
    "    #                     and operator_index_map[split_operator_name] >= split_operator_index\n",
    "    #                 ):\n",
    "    #                     # Do graph splitting.\n",
    "    #                     split_conf.pop(0)\n",
    "    #                     snode_dep.append({\"nodes\": {}, \"ref_nodes\": {}})\n",
    "    #                     ann = _recursion(\n",
    "    #                         anf.body,\n",
    "    #                         pipeline_mods,\n",
    "    #                         split_conf,\n",
    "    #                         constant_expr,\n",
    "    #                     )\n",
    "    #                     snode_dep.pop()\n",
    "    #                     dep_vars = get_dep_var(snode_dep)\n",
    "    #                     # When the nodes of the current subgraph are the depedency node of another\n",
    "    #                     # subgraph, we need to set them as the output of current subgraph.\n",
    "    #                     body = relay.Tuple(dep_vars) if len(dep_vars) > 1 else anf.var\n",
    "    #                     # when the operator of current subgraph uses previous subgraph constant\n",
    "    #                     # as the argument of a \"relay.expr.call\", such constant may become a free\n",
    "    #                     # varaible if the constant does not exist in the current subgraph.\n",
    "    #                     # merge the previous constant with current subgraph to avoid such issue.\n",
    "    #                     if constant_expr:\n",
    "    #                         ann = merge_constant_expr(constant_expr, ann)\n",
    "    #                     ann = run_opt_pass(ann, transform.ToGraphNormalForm())\n",
    "    #                     mod = tvm.IRModule.from_expr(ann)\n",
    "    #                     pipeline_mods.insert(0, mod)\n",
    "    #                     # Return the last node of the current subgraph.\n",
    "    #                     return tvm.relay.expr.Let(anf.var, value, body)\n",
    "    #         return tvm.relay.expr.Let(\n",
    "    #             anf.var,\n",
    "    #             value,\n",
    "    #             _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "    #         )\n",
    "    #     # Or End\n",
    "    #     else:\n",
    "    #         return anf\n",
    "\n",
    "    snode_dep = [{\"nodes\": {}, \"ref_nodes\": {}}]\n",
    "    pipeline_mods = []\n",
    "    operator_index_map = {}\n",
    "    # Used to tracking new input which caused by graph splitting.\n",
    "    new_input_idx = 0\n",
    "    constant_expr = None\n",
    "    subgraph_split_conf = split_conf.copy()\n",
    "    # Binding the parameters.\n",
    "    if params:\n",
    "        expr = build_module.bind_params_by_name(expr, params)\n",
    "    anf = run_opt_pass(expr, transform.ToANormalForm())\n",
    "    anf = run_opt_pass(anf, transform.InferType())\n",
    "    ann = _recursion(\n",
    "        anf,\n",
    "        pipeline_mods,\n",
    "        subgraph_split_conf,\n",
    "        constant_expr,\n",
    "    )\n",
    "    ann = run_opt_pass(ann.body, transform.ToGraphNormalForm())\n",
    "    mod = tvm.IRModule.from_expr(ann)\n",
    "    pipeline_mods.insert(0, mod)\n",
    "\n",
    "    return pipeline_mods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_split2(expr, split_conf, params=None):\n",
    "    \"\"\"Splitting the graph into a list of subgraphs\"\"\"\n",
    "\n",
    "    def get_dep_var(sub_var_dep):\n",
    "        return [var for var in sub_var_dep[len(sub_var_dep) - 1][\"ref_nodes\"]]\n",
    "\n",
    "    def parse_dependency(value, snode_dep, new_input_idx):\n",
    "        new_args = []\n",
    "        need_update = False\n",
    "        for var in value.args:\n",
    "            is_free_var = False\n",
    "            for dep in snode_dep[:-1]:\n",
    "                if var in dep[\"nodes\"]:\n",
    "                    # Mark the previous subgraph node as a dependency.\n",
    "                    dep[\"nodes\"][var] += 1\n",
    "                    dep[\"ref_nodes\"][var] = dep[\"nodes\"][var]\n",
    "                    # The var of this call is a free_var\n",
    "                    is_free_var = True\n",
    "            # if the var of this call is a free_var, recreate it and give it a fixed input name.\n",
    "            if is_free_var:\n",
    "                need_update = True\n",
    "                new_args.append(relay.var(f\"data_n_{new_input_idx}\", var.checked_type))\n",
    "                new_input_idx += 1\n",
    "            else:\n",
    "                new_args.append(var)\n",
    "        # if the 'tvm.relay.expr.Call' has a free_var, recreate it with new name as 'data_n_*'.\n",
    "        if need_update:\n",
    "            value = tvm.relay.expr.Call(\n",
    "                value.op, new_args, value.attrs, value.type_args, value.span\n",
    "            )\n",
    "        return value, snode_dep, new_input_idx\n",
    "\n",
    "    def merge_constant_expr(constant_expr, expr):\n",
    "        # merge constant express with a express\n",
    "        if not isinstance(constant_expr.body, tvm.relay.expr.Let):\n",
    "            return tvm.relay.expr.Let(constant_expr.var, constant_expr.value, expr)\n",
    "\n",
    "        return tvm.relay.expr.Let(\n",
    "            constant_expr.var, constant_expr.value, merge_constant_expr(constant_expr.body, expr)\n",
    "        )\n",
    "\n",
    "    def _recursion(anf, pipeline_mods, split_conf, constant_expr):\n",
    "        # Enumurate all operators of compute graph, then split the compute graph into a group of\n",
    "        # subgraph.\n",
    "        nonlocal operator_index_map\n",
    "        nonlocal new_input_idx\n",
    "        nonlocal snode_dep\n",
    "        # Get last element in snode_dep : current node's dependency\n",
    "        cur_node_dep = snode_dep[len(snode_dep) - 1]\n",
    "        # If function -> decouple\n",
    "        if isinstance(anf, tvm.relay.Function):\n",
    "            return tvm.relay.Function(\n",
    "                anf.params,\n",
    "                _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "                anf.ret_type,\n",
    "                anf.type_params,\n",
    "                anf.attrs,\n",
    "            )\n",
    "        # Function of Let\n",
    "        if isinstance(anf, tvm.relay.expr.Let):\n",
    "            value = anf.value\n",
    "            # record the constant expr to make sure all sugraphs can find correct constant.\n",
    "            if isinstance(value, tvm.relay.expr.Constant):\n",
    "                # cosntant_expr is initally None\n",
    "                if not constant_expr:\n",
    "                    constant_expr = tvm.relay.expr.Let(anf.var, value, anf.var)\n",
    "                else:\n",
    "                    constant_expr = tvm.relay.expr.Let(anf.var, value, constant_expr)\n",
    "            if isinstance(value, tvm.relay.expr.Call):\n",
    "                new_args = []\n",
    "                # build current var list\n",
    "                cur_node_dep[\"nodes\"][anf.var] = 0\n",
    "                # Get the dependency information of the nodes.\n",
    "                value, snode_dep, new_input_idx = parse_dependency(value, snode_dep, new_input_idx)\n",
    "                if isinstance(value.op, tvm.ir.Op):\n",
    "                    if value.op.name in operator_index_map:\n",
    "                        operator_index_map[value.op.name] += 1\n",
    "                    else:\n",
    "                        operator_index_map[value.op.name] = 0\n",
    "                    split_operator_name = split_conf[0][\"op_name\"] if split_conf else \"\"\n",
    "                    split_operator_index = split_conf[0][\"op_index\"] if split_conf else \"\"\n",
    "                    # if a operator name and repeating count in the network match with the values\n",
    "                    # of the 'split configuration', then this place is where we should do the\n",
    "                    # graph splitting.\n",
    "                    if (\n",
    "                        split_conf\n",
    "                        and split_operator_name in operator_index_map\n",
    "                        and operator_index_map[split_operator_name] >= split_operator_index\n",
    "                    ):\n",
    "                        # Do graph splitting.\n",
    "                        split_conf.pop(0)\n",
    "                        snode_dep.append({\"nodes\": {}, \"ref_nodes\": {}})\n",
    "                        ann = _recursion(\n",
    "                            anf.body,\n",
    "                            pipeline_mods,\n",
    "                            split_conf,\n",
    "                            constant_expr,\n",
    "                        )\n",
    "                        snode_dep.pop()\n",
    "                        dep_vars = get_dep_var(snode_dep)\n",
    "                        # When the nodes of the current subgraph are the depedency node of another\n",
    "                        # subgraph, we need to set them as the output of current subgraph.\n",
    "                        body = relay.Tuple(dep_vars) if len(dep_vars) > 1 else anf.var\n",
    "                        # when the operator of current subgraph uses previous subgraph constant\n",
    "                        # as the argument of a \"relay.expr.call\", such constant may become a free\n",
    "                        # varaible if the constant does not exist in the current subgraph.\n",
    "                        # merge the previous constant with current subgraph to avoid such issue.\n",
    "                        if constant_expr:\n",
    "                            ann = merge_constant_expr(constant_expr, ann)\n",
    "                        # ann = run_opt_pass(ann, transform.ToGraphNormalForm())\n",
    "                        # mod = tvm.IRModule.from_expr(ann)\n",
    "                        pipeline_mods.insert(0, ann)\n",
    "                        # Return the last node of the current subgraph.\n",
    "                        return tvm.relay.expr.Let(anf.var, value, body)\n",
    "            return tvm.relay.expr.Let(\n",
    "                anf.var,\n",
    "                value,\n",
    "                _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "            )\n",
    "        # Or End\n",
    "        else:\n",
    "            return anf\n",
    "    \n",
    "    def getting_inputs(mod):\n",
    "        name_hints = []\n",
    "        ann = run_opt_pass(mod.body, transform.ToGraphNormalForm())\n",
    "        mod = tvm.IRModule.from_expr(ann)\n",
    "        for param in mod['main'].params:\n",
    "            name_hints.append(param.name_hint)\n",
    "        return name_hints\n",
    "\n",
    "    def setting_outputs(anf, name_hints, outputs):\n",
    "        # Get last element in snode_dep : current node's dependency\n",
    "        # If function -> decouple\n",
    "        if isinstance(anf, tvm.relay.Function):\n",
    "            return tvm.relay.Function(\n",
    "                anf.params,\n",
    "                setting_outputs(anf.body, name_hints, outputs),\n",
    "                anf.ret_type,\n",
    "                anf.type_params,\n",
    "                anf.attrs,\n",
    "            )\n",
    "        # Function of Let\n",
    "        if isinstance(anf, tvm.relay.expr.Let):\n",
    "            value = anf.value\n",
    "            # print(anf.var.name_hint)\n",
    "            if anf.var.name_hint in name_hints:\n",
    "                outputs.append(anf)\n",
    "            # value, snode_dep, new_input_idx = parse_dependency(value, snode_dep, new_input_idx)\n",
    "            return tvm.relay.expr.Let(\n",
    "                anf.var,\n",
    "                value,\n",
    "                setting_outputs(anf.body, name_hints, outputs),\n",
    "            )\n",
    "        # Or End\n",
    "        else:\n",
    "            # new_map = \n",
    "            return anf\n",
    "\n",
    "    snode_dep = [{\"nodes\": {}, \"ref_nodes\": {}}]\n",
    "    pipeline_mods = []\n",
    "    operator_index_map = {}\n",
    "    # Used to tracking new input which caused by graph splitting.\n",
    "    new_input_idx = 0\n",
    "    constant_expr = None\n",
    "    subgraph_split_conf = split_conf.copy()\n",
    "    # Binding the parameters.\n",
    "    if params:\n",
    "        expr = build_module.bind_params_by_name(expr, params)\n",
    "    anf = run_opt_pass(expr, transform.ToANormalForm())\n",
    "    anf = run_opt_pass(anf, transform.InferType())\n",
    "    ann = _recursion(\n",
    "        anf,\n",
    "        pipeline_mods,\n",
    "        subgraph_split_conf,\n",
    "        constant_expr,\n",
    "    )\n",
    "    # ann = run_opt_pass(ann.body, transform.ToGraphNormalForm())\n",
    "    # mod = tvm.IRModule.from_expr(ann)\n",
    "    pipeline_mods.insert(0, ann.body)\n",
    "    inputs = []\n",
    "    for mod in pipeline_mods:\n",
    "        inputs.extend(getting_inputs(mod))\n",
    "    print(inputs)\n",
    "    total_outputs = []\n",
    "    for mod in pipeline_mods:\n",
    "        outputs = []\n",
    "        setting_outputs(mod, inputs, outputs)\n",
    "        total_outputs.append(outputs)\n",
    "    return pipeline_mods, total_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(expr, split_conf, params=None):\n",
    "    \"\"\"Splitting the graph into a list of subgraphs\"\"\"\n",
    "\n",
    "    def get_dep_var(sub_var_dep):\n",
    "        return [var for var in sub_var_dep[len(sub_var_dep) - 1][\"ref_nodes\"]]\n",
    "\n",
    "    def parse_dependency(value, snode_dep, new_input_idx):\n",
    "        new_args = []\n",
    "        need_update = False\n",
    "        for var in value.args:\n",
    "            is_free_var = False\n",
    "            for dep in snode_dep[:-1]:\n",
    "                if var in dep[\"nodes\"]:\n",
    "                    # Mark the previous subgraph node as a dependency.\n",
    "                    dep[\"nodes\"][var] += 1\n",
    "                    dep[\"ref_nodes\"][var] = dep[\"nodes\"][var]\n",
    "                    # The var of this call is a free_var\n",
    "                    is_free_var = True\n",
    "            # if the var of this call is a free_var, recreate it and give it a fixed input name.\n",
    "            if is_free_var:\n",
    "                need_update = True\n",
    "                new_args.append(relay.var(f\"data_n_{new_input_idx}\", var.checked_type))\n",
    "                new_input_idx += 1\n",
    "            else:\n",
    "                new_args.append(var)\n",
    "        # if the 'tvm.relay.expr.Call' has a free_var, recreate it with new name as 'data_n_*'.\n",
    "        if need_update:\n",
    "            value = tvm.relay.expr.Call(\n",
    "                value.op, new_args, value.attrs, value.type_args, value.span\n",
    "            )\n",
    "        return value, snode_dep, new_input_idx\n",
    "\n",
    "    def merge_constant_expr(constant_expr, expr):\n",
    "        # merge constant express with a express\n",
    "        if not isinstance(constant_expr.body, tvm.relay.expr.Let):\n",
    "            return tvm.relay.expr.Let(constant_expr.var, constant_expr.value, expr)\n",
    "\n",
    "        return tvm.relay.expr.Let(\n",
    "            constant_expr.var, constant_expr.value, merge_constant_expr(constant_expr.body, expr)\n",
    "        )\n",
    "\n",
    "    def _recursion(anf, pipeline_mods, split_conf, constant_expr):\n",
    "        # Enumurate all operators of compute graph, then split the compute graph into a group of\n",
    "        # subgraph.\n",
    "        nonlocal operator_index_map\n",
    "        nonlocal new_input_idx\n",
    "        nonlocal snode_dep\n",
    "        # Get last element in snode_dep : current node's dependency\n",
    "        cur_node_dep = snode_dep[len(snode_dep) - 1]\n",
    "        # If function -> decouple\n",
    "        if isinstance(anf, tvm.relay.Function):\n",
    "            return tvm.relay.Function(\n",
    "                anf.params,\n",
    "                _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "                anf.ret_type,\n",
    "                anf.type_params,\n",
    "                anf.attrs,\n",
    "            )\n",
    "        # Function of Let\n",
    "        if isinstance(anf, tvm.relay.expr.Let):\n",
    "            value = anf.value\n",
    "            # record the constant expr to make sure all sugraphs can find correct constant.\n",
    "            if isinstance(value, tvm.relay.expr.Constant):\n",
    "                # cosntant_expr is initally None\n",
    "                if not constant_expr:\n",
    "                    constant_expr = tvm.relay.expr.Let(anf.var, value, anf.var)\n",
    "                else:\n",
    "                    constant_expr = tvm.relay.expr.Let(anf.var, value, constant_expr)\n",
    "            if isinstance(value, tvm.relay.expr.Call):\n",
    "                print(anf.var.name_hint)\n",
    "                print(anf.var)\n",
    "                new_args = []\n",
    "                # build current var list\n",
    "                cur_node_dep[\"nodes\"][anf.var] = 0\n",
    "                # Get the dependency information of the nodes.\n",
    "                value, snode_dep, new_input_idx = parse_dependency(value, snode_dep, new_input_idx)\n",
    "                if isinstance(value.op, tvm.ir.Op):\n",
    "                    if value.op.name in operator_index_map:\n",
    "                        operator_index_map[value.op.name] += 1\n",
    "                    else:\n",
    "                        operator_index_map[value.op.name] = 0\n",
    "                    # split_operator_name = split_conf[0][\"op_name\"] if split_conf else \"\"\n",
    "                    # split_operator_index = split_conf[0][\"op_index\"] if split_conf else \"\"\n",
    "                    # if a operator name and repeating count in the network match with the values\n",
    "                    # of the 'split configuration', then this place is where we should do the\n",
    "                    # graph splitting.\n",
    "                    # if (\n",
    "                    #     split_conf\n",
    "                    #     and split_operator_name in operator_index_map\n",
    "                    #     and operator_index_map[split_operator_name] >= split_operator_index\n",
    "                    # ):\n",
    "                    #     # Do graph splitting.\n",
    "                    #     split_conf.pop(0)\n",
    "                    #     snode_dep.append({\"nodes\": {}, \"ref_nodes\": {}})\n",
    "                    #     ann = _recursion(\n",
    "                    #         anf.body,\n",
    "                    #         pipeline_mods,\n",
    "                    #         split_conf,\n",
    "                    #         constant_expr,\n",
    "                    #     )\n",
    "                    #     snode_dep.pop()\n",
    "                    #     dep_vars = get_dep_var(snode_dep)\n",
    "                    #     # When the nodes of the current subgraph are the depedency node of another\n",
    "                    #     # subgraph, we need to set them as the output of current subgraph.\n",
    "                    #     body = relay.Tuple(dep_vars) if len(dep_vars) > 1 else anf.var\n",
    "                    #     # when the operator of current subgraph uses previous subgraph constant\n",
    "                    #     # as the argument of a \"relay.expr.call\", such constant may become a free\n",
    "                    #     # varaible if the constant does not exist in the current subgraph.\n",
    "                    #     # merge the previous constant with current subgraph to avoid such issue.\n",
    "                    #     if constant_expr:\n",
    "                    #         ann = merge_constant_expr(constant_expr, ann)\n",
    "                    #     ann = run_opt_pass(ann, transform.ToGraphNormalForm())\n",
    "                    #     mod = tvm.IRModule.from_expr(ann)\n",
    "                    #     pipeline_mods.insert(0, mod)\n",
    "                    #     # Return the last node of the current subgraph.\n",
    "                        # return tvm.relay.expr.Let(anf.var, value, body)\n",
    "            return tvm.relay.expr.Let(\n",
    "                anf.var,\n",
    "                value,\n",
    "                _recursion(anf.body, pipeline_mods, split_conf, constant_expr),\n",
    "            )\n",
    "        # Or End\n",
    "        else:\n",
    "            return anf\n",
    "\n",
    "    snode_dep = [{\"nodes\": {}, \"ref_nodes\": {}}]\n",
    "    pipeline_mods = []\n",
    "    operator_index_map = {}\n",
    "    # Used to tracking new input which caused by graph splitting.\n",
    "    new_input_idx = 0\n",
    "    constant_expr = None\n",
    "    subgraph_split_conf = split_conf.copy()\n",
    "    # Binding the parameters.\n",
    "    if params:\n",
    "        expr = build_module.bind_params_by_name(expr, params)\n",
    "    anf = run_opt_pass(expr, transform.ToANormalForm())\n",
    "    anf = run_opt_pass(anf, transform.InferType())\n",
    "    ann = _recursion(\n",
    "        anf,\n",
    "        pipeline_mods,\n",
    "        subgraph_split_conf,\n",
    "        constant_expr,\n",
    "    )\n",
    "    ann = run_opt_pass(ann.body, transform.ToGraphNormalForm())\n",
    "    mod = tvm.IRModule.from_expr(ann)\n",
    "    pipeline_mods.insert(0, mod)\n",
    "    return pipeline_mods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_outputs(anf, outputs):\n",
    "    # Get last element in snode_dep : current node's dependency\n",
    "    # If function -> decouple\n",
    "    if isinstance(anf, tvm.relay.Function):\n",
    "        return tvm.relay.Function(\n",
    "            anf.params,\n",
    "            setting_outputs(anf.body, outputs),\n",
    "            anf.ret_type,\n",
    "            anf.type_params,\n",
    "            anf.attrs,\n",
    "        )\n",
    "    # Function of Let\n",
    "    if isinstance(anf, tvm.relay.expr.Let):\n",
    "        value = anf.value\n",
    "        # print(anf.var.name_hint)\n",
    "        # if anf.var.name_hint in name_hints:\n",
    "            # outputs.append(anf)\n",
    "        # value, snode_dep, new_input_idx = parse_dependency(value, snode_dep, new_input_idx)\n",
    "        return tvm.relay.expr.Let(\n",
    "            anf.var,\n",
    "            value,\n",
    "            setting_outputs(anf.body, outputs),\n",
    "        )\n",
    "    # Or End\n",
    "    else:\n",
    "        new_outputs = []\n",
    "        for o in outputs:\n",
    "            new_outputs.append(o.var)\n",
    "        # for o in outputs:\n",
    "        #     new_outputs.append(tvm.relay.expr.Let(\n",
    "        #             o.var,\n",
    "        #             o.value,\n",
    "        #             o.body,\n",
    "        #     ))\n",
    "        new_map = tvm.relay.expr.Tuple(new_outputs)\n",
    "        return new_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetOutputCallback(DFPatternCallback):\n",
    "    def __init__(self, name_hints=[],require_type=False):\n",
    "        super().__init__(require_type)\n",
    "        super().__init__(rewrite_once=True)\n",
    "\n",
    "        self.pattern = wildcard()\n",
    "\n",
    "    def callback(self, pre, post, node_map):\n",
    "        try:\n",
    "            print(type(pre))\n",
    "            print(pre.name_hint)\n",
    "        except:\n",
    "            pass\n",
    "        return pre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./simple_model.h5\"\n",
    "input_data = np.random.normal(0,1,(1,256,256,3)).astype(np.float32)\n",
    "model_keras = tf.keras.models.load_model(base_path)\n",
    "# tvm result\n",
    "input_data = input_data.transpose([0, 3, 1, 2])\n",
    "shape_dict = {\"input_1\": input_data.shape}\n",
    "mod, params = relay.frontend.from_keras(model_keras, shape_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "[CallNode(Op(concatenate), [Tuple([CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d_transpose), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [Var(input_1, ty=TensorType([1, 3, 256, 256], float32)), Var(_param_1, ty=TensorType([16, 3, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x37677f8), []), Var(_param_2, ty=TensorType([16], float32))], relay.attrs.BiasAddAttrs(0x311e1cc8), [])], relay.attrs.MaxPool2DAttrs(0x3767468), []), Var(_param_3, ty=TensorType([32, 16, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311d4118), []), Var(_param_4, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x6183038), [])], relay.attrs.MaxPool2DAttrs(0x311c5df8), []), Var(_param_5, ty=TensorType([64, 32, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311b22e8), []), Var(_param_6, ty=TensorType([64], float32))], relay.attrs.BiasAddAttrs(0x311bf0b8), [])], relay.attrs.MaxPool2DAttrs(0x311c7bd8), []), Var(_param_7, ty=TensorType([64, 32, 3, 3], float32))], relay.attrs.Conv2DTransposeAttrs(0x61839a8), []), Var(_param_8, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x31245098), []), CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [Var(input_1, ty=TensorType([1, 3, 256, 256], float32)), Var(_param_1, ty=TensorType([16, 3, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x37677f8), []), Var(_param_2, ty=TensorType([16], float32))], relay.attrs.BiasAddAttrs(0x311e1cc8), [])], relay.attrs.MaxPool2DAttrs(0x3767468), []), Var(_param_3, ty=TensorType([32, 16, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311d4118), []), Var(_param_4, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x6183038), [])], relay.attrs.MaxPool2DAttrs(0x311c5df8), []), Var(_param_5, ty=TensorType([64, 32, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311b22e8), []), Var(_param_6, ty=TensorType([64], float32))], relay.attrs.BiasAddAttrs(0x311bf0b8), [])])], relay.attrs.ConcatenateAttrs(0x31246868), []), CallNode(Op(concatenate), [Tuple([CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d_transpose), [CallNode(Op(concatenate), [Tuple([CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d_transpose), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [Var(input_1, ty=TensorType([1, 3, 256, 256], float32)), Var(_param_1, ty=TensorType([16, 3, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x37677f8), []), Var(_param_2, ty=TensorType([16], float32))], relay.attrs.BiasAddAttrs(0x311e1cc8), [])], relay.attrs.MaxPool2DAttrs(0x3767468), []), Var(_param_3, ty=TensorType([32, 16, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311d4118), []), Var(_param_4, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x6183038), [])], relay.attrs.MaxPool2DAttrs(0x311c5df8), []), Var(_param_5, ty=TensorType([64, 32, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311b22e8), []), Var(_param_6, ty=TensorType([64], float32))], relay.attrs.BiasAddAttrs(0x311bf0b8), [])], relay.attrs.MaxPool2DAttrs(0x311c7bd8), []), Var(_param_7, ty=TensorType([64, 32, 3, 3], float32))], relay.attrs.Conv2DTransposeAttrs(0x61839a8), []), Var(_param_8, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x31245098), []), CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [Var(input_1, ty=TensorType([1, 3, 256, 256], float32)), Var(_param_1, ty=TensorType([16, 3, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x37677f8), []), Var(_param_2, ty=TensorType([16], float32))], relay.attrs.BiasAddAttrs(0x311e1cc8), [])], relay.attrs.MaxPool2DAttrs(0x3767468), []), Var(_param_3, ty=TensorType([32, 16, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311d4118), []), Var(_param_4, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x6183038), [])], relay.attrs.MaxPool2DAttrs(0x311c5df8), []), Var(_param_5, ty=TensorType([64, 32, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311b22e8), []), Var(_param_6, ty=TensorType([64], float32))], relay.attrs.BiasAddAttrs(0x311bf0b8), [])])], relay.attrs.ConcatenateAttrs(0x31246868), []), Var(_param_9, ty=TensorType([96, 16, 3, 3], float32))], relay.attrs.Conv2DTransposeAttrs(0x311b3318), []), Var(_param_10, ty=TensorType([16], float32))], relay.attrs.BiasAddAttrs(0x3b042f8), []), CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [CallNode(Op(nn.max_pool2d), [CallNode(Op(nn.bias_add), [CallNode(Op(nn.conv2d), [Var(input_1, ty=TensorType([1, 3, 256, 256], float32)), Var(_param_1, ty=TensorType([16, 3, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x37677f8), []), Var(_param_2, ty=TensorType([16], float32))], relay.attrs.BiasAddAttrs(0x311e1cc8), [])], relay.attrs.MaxPool2DAttrs(0x3767468), []), Var(_param_3, ty=TensorType([32, 16, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x311d4118), []), Var(_param_4, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x6183038), [])])], relay.attrs.ConcatenateAttrs(0x31195e48), [])]\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "out = quantize(mod, 2)\n",
    "split_config = [{\"op_name\": \"annotation.stop_fusion\", \"op_index\": 0}]\n",
    "# print(mod['main'])\n",
    "subgraphs, outputs = graph_split2(out, split_config, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.var.name_hint for o in outputs[1]]\n",
    "outputs[0][0].var.name_hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "modmod = setting_outputs(subgraphs[0], outputs[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = run_opt_pass(modmod, transform.ToGraphNormalForm())\n",
    "out = tvm.IRModule.from_expr(ann)['main']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Var(x_0, ty=TensorType([16, 3, 3, 3], float32)), Var(x_8, ty=TensorType([1, 32, 128, 128], float32)), Var(x_20, ty=TensorType([1, 96, 64, 64], float32))]\n"
     ]
    }
   ],
   "source": [
    "print([o.var for o in outputs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = subgraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = subgraphs[0]\n",
    "ann = run_opt_pass(out, transform.ToGraphNormalForm())\n",
    "out = tvm.IRModule.from_expr(ann)['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%input_1: Tensor[(1, 3, 256, 256), float32] /* ty=Tensor[(1, 3, 256, 256), float32] */) {\n",
       "  %0 = nn.conv2d(%input_1, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n",
       "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n",
       "  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 16, 128i64, 128i64), float32] */;\n",
       "  %3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(32, 16, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n",
       "  %4 = nn.bias_add(%3, meta[relay.Constant][3] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n",
       "  %5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n",
       "  %6 = nn.conv2d(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n",
       "  %7 = nn.bias_add(%6, meta[relay.Constant][5] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n",
       "  %8 = nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n",
       "  %9 = nn.conv2d_transpose(%8, meta[relay.Constant][6] /* ty=Tensor[(64, 32, 3, 3), float32] */, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n",
       "  %10 = nn.bias_add(%9, meta[relay.Constant][7] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n",
       "  %11 = (%10, %7) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n",
       "  concatenate(%11, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvm.IRModule.from_expr(run_opt_pass(subgraphs[0], transform.ToGraphNormalForm()))['main']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fn (%x_20: Tensor[(1, 96, 64, 64), float32] /* ty=Tensor[(1, 96, 64, 64), float32] */, %x_8: Tensor[(1, 32, 128, 128), float32] /* ty=Tensor[(1, 32, 128, 128), float32] */) {\n",
       "  %0 = nn.conv2d_transpose(%x_20, meta[relay.Constant][0] /* ty=Tensor[(96, 16, 3, 3), float32] */, channels=16, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 16, 128, 128), float32] */;\n",
       "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 128, 128), float32] */;\n",
       "  %2 = (%1, %x_8) /* ty=(Tensor[(1, 16, 128, 128), float32], Tensor[(1, 32, 128, 128), float32]) */;\n",
       "  %3 = concatenate(%2, axis=1) /* ty=Tensor[(1, 48, 128, 128), float32] */;\n",
       "  %4 = nn.conv2d_transpose(%3, meta[relay.Constant][2] /* ty=Tensor[(48, 1, 3, 3), float32] */, channels=1, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 1, 256, 256), float32] */;\n",
       "  nn.bias_add(%4, meta[relay.Constant][3] /* ty=Tensor[(1), float32] */) /* ty=Tensor[(1, 1, 256, 256), float32] */\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvm.IRModule.from_expr(run_opt_pass(subgraphs[1], transform.ToGraphNormalForm()))['main']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = subgraphs[0]\n",
    "ann = run_opt_pass(out, transform.ToGraphNormalForm())\n",
    "out = tvm.IRModule.from_expr(ann)['main']\n",
    "# outs = outputs[0][1]\n",
    "outs = run_opt_pass(outputs[0][1], transform.ToGraphNormalForm())\n",
    "outs = tvm.IRModule.from_expr(outs)['main']\n",
    "\n",
    "# outnew = run_opt_pass(out_1, transform.ToGraphNormalForm())\n",
    "# outnew = tvm.IRModule.from_expr(outnew)['main']\n",
    "\n",
    "# out1 = relay.Function(params, relay.Tuple([outs, out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = run_opt_pass(subgraph, transform.ToGraphNormalForm())\n",
    "submod = tvm.IRModule.from_expr(ann)['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = subgraphs[0]\n",
    "ann = run_opt_pass(out, transform.ToGraphNormalForm())\n",
    "out = tvm.IRModule.from_expr(ann)['main']\n",
    "outnew = run_opt_pass(out_1, transform.ToGraphNormalForm())\n",
    "outnew = tvm.IRModule.from_expr(outnew)['main']\n",
    "\n",
    "out1 = relay.Function(out.params, relay.Tuple([outnew, out]), out.ret_type, out.type_params, out.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x_0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0].var.name_hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_8 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_9 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_10 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_11 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_12 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_13 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_14 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_15 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_16 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_17 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_18 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_19 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_20 is bound more than once, this is not valid IR\n",
      "[23:58:33] /home/jd/workspace/tvm-v0.9.0/src/relay/analysis/well_formed.cc:49: The IR is not well formed with: The variable x_20 is bound more than once, this is not valid IR\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  6: TVMFuncCall\n  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)>::AssignTypedLambda<tvm::IRModule (*)(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)>(tvm::IRModule (*)(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  4: tvm::IRModule::FromExpr(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)\n  3: tvm::IRModule::FromExprInContext(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&, std::unordered_set<tvm::runtime::String, std::hash<tvm::runtime::String>, std::equal_to<tvm::runtime::String>, std::allocator<tvm::runtime::String> >)\n  2: tvm::IRModuleNode::Add(tvm::GlobalVar const&, tvm::BaseFunc const&, bool)\n  1: tvm::WarnIfMalformed(tvm::IRModule const&, tvm::relay::Function)\n  0: tvm::relay::DeDup(tvm::RelayExpr const&)\n  File \"/home/jd/workspace/tvm-v0.9.0/src/relay/transforms/de_duplicate.cc\", line 113\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (WellFormed(e)) is false: #[version = \"0.0.5\"]\nfn (%input_1: Tensor[(1, 3, 256, 256), float32] /* ty=Tensor[(1, 3, 256, 256), float32] */) {\n  %0 = (\n    let %x_0: Tensor[(16, 3, 3, 3), float32] /* ty=Tensor[(16, 3, 3, 3), float32] */ = meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */;\n    let %x_1: Tensor[(1, 16, 256, 256), float32] /* ty=Tensor[(1, 16, 256, 256), float32] */ = nn.conv2d(%input_1, %x_0, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n    let %x_2: Tensor[(16), float32] /* ty=Tensor[(16), float32] */ = meta[relay.Constant][1] /* ty=Tensor[(16), float32] */;\n    let %x_3: Tensor[(1, 16, 256, 256), float32] /* ty=Tensor[(1, 16, 256, 256), float32] */ = nn.bias_add(%x_1, %x_2) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n    let %x_4: Tensor[(1, 16, 128i64, 128i64), float32] /* ty=Tensor[(1, 16, 128i64, 128i64), float32] */ = nn.max_pool2d(%x_3, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 16, 128i64, 128i64), float32] */;\n    let %x_5: Tensor[(32, 16, 3, 3), float32] /* ty=Tensor[(32, 16, 3, 3), float32] */ = meta[relay.Constant][2] /* ty=Tensor[(32, 16, 3, 3), float32] */;\n    let %x_6: Tensor[(1, 32, 128, 128), float32] /* ty=Tensor[(1, 32, 128, 128), float32] */ = nn.conv2d(%x_4, %x_5, padding=[1i64, 1i64, 1i64, 1i64], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n    let %x_7: Tensor[(32), float32] /* ty=Tensor[(32), float32] */ = meta[relay.Constant][3] /* ty=Tensor[(32), float32] */;\n    let %x_8: Tensor[(1, 32, 128, 128), float32] /* ty=Tensor[(1, 32, 128, 128), float32] */ = nn.bias_add(%x_6, %x_7) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n    let %x_9: Tensor[(1, 32, 64i64, 64i64), float32] /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */ = nn.max_pool2d(%x_8, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n    let %x_10: Tensor[(64, 32, 3, 3), float32] /* ty=Tensor[(64, 32, 3, 3), float32] */ = meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */;\n    let %x_11: Tensor[(1, 64, 64, 64), float32] /* ty=Tensor[(1, 64, 64, 64), float32] */ = nn.conv2d(%x_9, %x_10, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n    let %x_12: Tensor[(64), float32] /* ty=Tensor[(64), float32] */ = meta[relay.Constant][5] /* ty=Tensor[(64), float32] */;\n    let %x_13: Tensor[(1, 64, 64, 64), float32] /* ty=Tensor[(1, 64, 64, 64), float32] */ = nn.bias_add(%x_11, %x_12) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n    let %x_14: Tensor[(1, 64, 32i64, 32i64), float32] /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */ = nn.max_pool2d(%x_13, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n    let %x_15: Tensor[(64, 32, 3, 3), float32] /* ty=Tensor[(64, 32, 3, 3), float32] */ = meta[relay.Constant][6] /* ty=Tensor[(64, 32, 3, 3), float32] */;\n    let %x_16: Tensor[(1, 32, 64, 64), float32] /* ty=Tensor[(1, 32, 64, 64), float32] */ = nn.conv2d_transpose(%x_14, %x_15, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n    let %x_17: Tensor[(32), float32] /* ty=Tensor[(32), float32] */ = meta[relay.Constant][7] /* ty=Tensor[(32), float32] */;\n    let %x_18: Tensor[(1, 32, 64, 64), float32] /* ty=Tensor[(1, 32, 64, 64), float32] */ = nn.bias_add(%x_16, %x_17) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n    let %x_19: (Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */ = (%x_18, %x_13) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n    let %x_20: Tensor[(1, 96, 64, 64), float32] /* ty=Tensor[(1, 96, 64, 64), float32] */ = concatenate(%x_19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n    %x_20\n  );\n  %1 = (\n    let %x_8-malformed-ir = nn.bias_add(%x_6, %x_7) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n    let %x_9-malformed-ir = nn.max_pool2d(%x_8, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n    let %x_10-malformed-ir = meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */;\n    let %x_11-malformed-ir = nn.conv2d(%x_9, %x_10, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n    let %x_12-malformed-ir = meta[relay.Constant][5] /* ty=Tensor[(64), float32] */;\n    let %x_13-malformed-ir = nn.bias_add(%x_11, %x_12) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n    let %x_14-malformed-ir = nn.max_pool2d(%x_13, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n    let %x_15-malformed-ir = meta[relay.Constant][6] /* ty=Tensor[(64, 32, 3, 3), float32] */;\n    let %x_16-malformed-ir = nn.conv2d_transpose(%x_14, %x_15, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n    let %x_17-malformed-ir = meta[relay.Constant][7] /* ty=Tensor[(32), float32] */;\n    let %x_18-malformed-ir = nn.bias_add(%x_16, %x_17) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n    let %x_19-malformed-ir = (%x_18, %x_13) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n    let %x_20-malformed-ir = concatenate(%x_19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n    %x_20\n  );\n  %2 = (\n    let %x_20-malformed-ir = concatenate(%x_19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n    %x_20\n  );\n  (%0, %1, %2)\n}\n/* For debugging purposes the metadata section has been omitted.\n * If you would like to see the full metadata section you can set the \n * option to `True` when invoking `astext`. \n */",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m out \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39mTuple(outputs[\u001b[39m0\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m ann \u001b[39m=\u001b[39m run_opt_pass(out, transform\u001b[39m.\u001b[39;49mToGraphNormalForm())\n\u001b[1;32m      3\u001b[0m out \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mIRModule\u001b[39m.\u001b[39mfrom_expr(ann)[\u001b[39m'\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/relay/testing/__init__.py:53\u001b[0m, in \u001b[0;36mrun_opt_pass\u001b[0;34m(expr, opt_pass, import_prelude)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_opt_pass\u001b[39m(expr, opt_pass, import_prelude\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     52\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(opt_pass, tvm\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mPass)\n\u001b[0;32m---> 53\u001b[0m     mod \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39;49mIRModule\u001b[39m.\u001b[39;49mfrom_expr(expr)\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m import_prelude:\n\u001b[1;32m     55\u001b[0m         Prelude(mod)\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/ir/module.py:243\u001b[0m, in \u001b[0;36mIRModule.from_expr\u001b[0;34m(expr, functions, type_defs)\u001b[0m\n\u001b[1;32m    241\u001b[0m funcs \u001b[39m=\u001b[39m functions \u001b[39mif\u001b[39;00m functions \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m    242\u001b[0m defs \u001b[39m=\u001b[39m type_defs \u001b[39mif\u001b[39;00m type_defs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m--> 243\u001b[0m \u001b[39mreturn\u001b[39;00m _ffi_api\u001b[39m.\u001b[39;49mModule_FromExpr(expr, funcs, defs)\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  6: TVMFuncCall\n  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)>::AssignTypedLambda<tvm::IRModule (*)(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)>(tvm::IRModule (*)(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  4: tvm::IRModule::FromExpr(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)\n  3: tvm::IRModule::FromExprInContext(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&, std::unordered_set<tvm::runtime::String, std::hash<tvm::runtime::String>, std::equal_to<tvm::runtime::String>, std::allocator<tvm::runtime::String> >)\n  2: tvm::IRModuleNode::Add(tvm::GlobalVar const&, tvm::BaseFunc const&, bool)\n  1: tvm::WarnIfMalformed(tvm::IRModule const&, tvm::relay::Function)\n  0: tvm::relay::DeDup(tvm::RelayExpr const&)\n  File \"/home/jd/workspace/tvm-v0.9.0/src/relay/transforms/de_duplicate.cc\", line 113\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n  Check failed: (WellFormed(e)) is false: #[version = \"0.0.5\"]\nfn (%input_1: Tensor[(1, 3, 256, 256), float32] /* ty=Tensor[(1, 3, 256, 256), float32] */) {\n  %0 = (\n    let %x_0: Tensor[(16, 3, 3, 3), float32] /* ty=Tensor[(16, 3, 3, 3), float32] */ = meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */;\n    let %x_1: Tensor[(1, 16, 256, 256), float32] /* ty=Tensor[(1, 16, 256, 256), float32] */ = nn.conv2d(%input_1, %x_0, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n    let %x_2: Tensor[(16), float32] /* ty=Tensor[(16), float32] */ = meta[relay.Constant][1] /* ty=Tensor[(16), float32] */;\n    let %x_3: Tensor[(1, 16, 256, 256), float32] /* ty=Tensor[(1, 16, 256, 256), float32] */ = nn.bias_add(%x_1, %x_2) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n    let %x_4: Tensor[(1, 16, 128i64, 128i64), float32] /* ty=Tensor[(1, 16, 128i64, 128i64), float32] */ = nn.max_pool2d(%x_3, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 16, 128i64, 128i64), float32] */;\n    let %x_5: Tensor[(32, 16, 3, 3), float32] /* ty=Tensor[(32, 16, 3, 3), float32] */ = meta[relay.Constant][2] /* ty=Tensor[(32, 16, 3, 3), float32] */;\n    let %x_6: Tensor[(1, 32, 128, 128), float32] /* ty=Tensor[(1, 32, 128, 128), float32] */ = nn.conv2d(%x_4, %x_5, padding=[1i64, 1i64, 1i64, 1i64], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n    let %x_7: Tensor[(32), float32] /* ty=Tensor[(32), float32] */ = meta[relay.Constant][3] /* ty=Tensor[(32), float32] */;\n    let %x_8: Tensor[(1, 32, 128, 128), float32] /* ty=Tensor[(1, 32, 128, 128), float32] */ = nn.bias_add(%x_6, %x_7) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n    let %x_9: Tensor[(1, 32, 64i64, 64i64), float32] /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */ = nn.max_pool2d(%x_8, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n    let %x_10: Tensor[(64, 32, 3, 3), float32] /* ty=Tensor[(64, 32, 3, 3), float32] */ = meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */;\n    let %x_11: Tensor[(1, 64, 64, 64), float32] /* ty=Tensor[(1, 64, 64, 64), float32] */ = nn.conv2d(%x_9, %x_10, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n    let %x_12: Tensor[(64), float32] /* ty=Tensor[(64), float32] */ = meta[relay.Constant][5] /* ty=Tensor[(64), float32] */;\n    let %x_13: Tensor[(1, 64, 64, 64), float32] /* ty=Tensor[(1, 64, 64, 64), float32] */ = nn.bias_add(%x_11, %x_12) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n    let %x_14: Tensor[(1, 64, 32i64, 32i64), float32] /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */ = nn.max_pool2d(%x_13, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n    let %x_15: Tensor[(64, 32, 3, 3), float32] /* ty=Tensor[(64, 32, 3, 3), float32] */ = meta[relay.Constant][6] /* ty=Tensor[(64, 32, 3, 3), float32] */;\n    let %x_16: Tensor[(1, 32, 64, 64), float32] /* ty=Tensor[(1, 32, 64, 64), float32] */ = nn.conv2d_transpose(%x_14, %x_15, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n    let %x_17: Tensor[(32), float32] /* ty=Tensor[(32), float32] */ = meta[relay.Constant][7] /* ty=Tensor[(32), float32] */;\n    let %x_18: Tensor[(1, 32, 64, 64), float32] /* ty=Tensor[(1, 32, 64, 64), float32] */ = nn.bias_add(%x_16, %x_17) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n    let %x_19: (Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */ = (%x_18, %x_13) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n    let %x_20: Tensor[(1, 96, 64, 64), float32] /* ty=Tensor[(1, 96, 64, 64), float32] */ = concatenate(%x_19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n    %x_20\n  );\n  %1 = (\n    let %x_8-malformed-ir = nn.bias_add(%x_6, %x_7) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n    let %x_9-malformed-ir = nn.max_pool2d(%x_8, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n    let %x_10-malformed-ir = meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */;\n    let %x_11-malformed-ir = nn.conv2d(%x_9, %x_10, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n    let %x_12-malformed-ir = meta[relay.Constant][5] /* ty=Tensor[(64), float32] */;\n    let %x_13-malformed-ir = nn.bias_add(%x_11, %x_12) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n    let %x_14-malformed-ir = nn.max_pool2d(%x_13, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n    let %x_15-malformed-ir = meta[relay.Constant][6] /* ty=Tensor[(64, 32, 3, 3), float32] */;\n    let %x_16-malformed-ir = nn.conv2d_transpose(%x_14, %x_15, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n    let %x_17-malformed-ir = meta[relay.Constant][7] /* ty=Tensor[(32), float32] */;\n    let %x_18-malformed-ir = nn.bias_add(%x_16, %x_17) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n    let %x_19-malformed-ir = (%x_18, %x_13) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n    let %x_20-malformed-ir = concatenate(%x_19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n    %x_20\n  );\n  %2 = (\n    let %x_20-malformed-ir = concatenate(%x_19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n    %x_20\n  );\n  (%0, %1, %2)\n}\n/* For debugging purposes the metadata section has been omitted.\n * If you would like to see the full metadata section you can set the \n * option to `True` when invoking `astext`. \n */"
     ]
    }
   ],
   "source": [
    "out = relay.Tuple(outputs[0])\n",
    "ann = run_opt_pass(out, transform.ToGraphNormalForm())\n",
    "out = tvm.IRModule.from_expr(ann)['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  3: TVMFuncCall\n  2: tvm::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const [clone .isra.0]\n  1: tvm::IRModuleNode::Add(tvm::GlobalVar const&, tvm::BaseFunc const&, bool)\n  0: tvm::WarnIfMalformed(tvm::IRModule const&, tvm::relay::Function)\n  File \"/home/jd/workspace/tvm-v0.9.0/src/ir/module.cc\", line 191\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: fv.size() == 0 (3 vs. 0) : Function:\nfn (%input_1: Tensor[(1, 3, 256, 256), float32] /* ty=Tensor[(1, 3, 256, 256), float32] */) {\n  %0 = nn.conv2d(%input_1, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 16, 128i64, 128i64), float32] */;\n  %3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(32, 16, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n  %4 = nn.bias_add(%3, meta[relay.Constant][3] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n  %5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n  %6 = nn.conv2d(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n  %7 = nn.bias_add(%6, meta[relay.Constant][5] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n  %8 = nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n  %9 = nn.conv2d_transpose(%8, meta[relay.Constant][6] /* ty=Tensor[(64, 32, 3, 3), float32] */, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n  %10 = nn.bias_add(%9, meta[relay.Constant][7] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n  %11 = (%10, %7) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n  free_var %x_6: Tensor[(1, 32, 128, 128), float32] /* ty=Tensor[(1, 32, 128, 128), float32] */;\n  free_var %x_7: Tensor[(32), float32] /* ty=Tensor[(32), float32] */;\n  %12 = nn.bias_add(%x_6, %x_7) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n  %13 = nn.max_pool2d(%12, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n  %14 = nn.conv2d(%13, meta[relay.Constant][8] /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n  %15 = nn.bias_add(%14, meta[relay.Constant][9] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n  %16 = nn.max_pool2d(%15, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n  %17 = nn.conv2d_transpose(%16, meta[relay.Constant][10] /* ty=Tensor[(64, 32, 3, 3), float32] */, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n  %18 = nn.bias_add(%17, meta[relay.Constant][11] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n  %19 = (%18, %15) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n  free_var %x_19: (Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n  %20 = concatenate(%11, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n  %21 = concatenate(%19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n  %22 = concatenate(%x_19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n  (%20, %21, %22)\n}\n\ncontains free variables: [Var(x_6, ty=TensorType([1, 32, 128, 128], float32)), Var(x_7, ty=TensorType([32], float32)), Var(x_19, ty=TupleTypeNode([TensorType([1, 32, 64, 64], float32), TensorType([1, 64, 64, 64], float32)]))]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m out1 \u001b[39m=\u001b[39m relay\u001b[39m.\u001b[39mFunction(out\u001b[39m.\u001b[39mparams, relay\u001b[39m.\u001b[39mTuple(outnew), out\u001b[39m.\u001b[39mret_type, out\u001b[39m.\u001b[39mtype_params, out\u001b[39m.\u001b[39mattrs)\n\u001b[1;32m      6\u001b[0m mod \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mIRModule()\n\u001b[0;32m----> 7\u001b[0m mod[\u001b[39m\"\u001b[39;49m\u001b[39mmain\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39m out1\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/ir/module.py:75\u001b[0m, in \u001b[0;36mIRModule.__setitem__\u001b[0;34m(self, var, val)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, var, val):\n\u001b[1;32m     65\u001b[0m     \u001b[39m\"\"\"Add a mapping to the module.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m        The value.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add(var, val, \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/ir/module.py:84\u001b[0m, in \u001b[0;36mIRModule._add\u001b[0;34m(self, var, val, update)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m             var \u001b[39m=\u001b[39m _expr\u001b[39m.\u001b[39mGlobalVar(var)\n\u001b[0;32m---> 84\u001b[0m     _ffi_api\u001b[39m.\u001b[39;49mModule_Add(\u001b[39mself\u001b[39;49m, var, val, update)\n\u001b[1;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(val, _ty\u001b[39m.\u001b[39mType)\n",
      "File \u001b[0;32m~/workspace/tvm-v0.9.0/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  3: TVMFuncCall\n  2: tvm::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const [clone .isra.0]\n  1: tvm::IRModuleNode::Add(tvm::GlobalVar const&, tvm::BaseFunc const&, bool)\n  0: tvm::WarnIfMalformed(tvm::IRModule const&, tvm::relay::Function)\n  File \"/home/jd/workspace/tvm-v0.9.0/src/ir/module.cc\", line 191\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: fv.size() == 0 (3 vs. 0) : Function:\nfn (%input_1: Tensor[(1, 3, 256, 256), float32] /* ty=Tensor[(1, 3, 256, 256), float32] */) {\n  %0 = nn.conv2d(%input_1, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n  %2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 16, 128i64, 128i64), float32] */;\n  %3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(32, 16, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n  %4 = nn.bias_add(%3, meta[relay.Constant][3] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n  %5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n  %6 = nn.conv2d(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n  %7 = nn.bias_add(%6, meta[relay.Constant][5] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n  %8 = nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n  %9 = nn.conv2d_transpose(%8, meta[relay.Constant][6] /* ty=Tensor[(64, 32, 3, 3), float32] */, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n  %10 = nn.bias_add(%9, meta[relay.Constant][7] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n  %11 = (%10, %7) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n  free_var %x_6: Tensor[(1, 32, 128, 128), float32] /* ty=Tensor[(1, 32, 128, 128), float32] */;\n  free_var %x_7: Tensor[(32), float32] /* ty=Tensor[(32), float32] */;\n  %12 = nn.bias_add(%x_6, %x_7) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n  %13 = nn.max_pool2d(%12, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n  %14 = nn.conv2d(%13, meta[relay.Constant][8] /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n  %15 = nn.bias_add(%14, meta[relay.Constant][9] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n  %16 = nn.max_pool2d(%15, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n  %17 = nn.conv2d_transpose(%16, meta[relay.Constant][10] /* ty=Tensor[(64, 32, 3, 3), float32] */, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n  %18 = nn.bias_add(%17, meta[relay.Constant][11] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n  %19 = (%18, %15) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n  free_var %x_19: (Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n  %20 = concatenate(%11, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n  %21 = concatenate(%19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n  %22 = concatenate(%x_19, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n  (%20, %21, %22)\n}\n\ncontains free variables: [Var(x_6, ty=TensorType([1, 32, 128, 128], float32)), Var(x_7, ty=TensorType([32], float32)), Var(x_19, ty=TupleTypeNode([TensorType([1, 32, 64, 64], float32), TensorType([1, 64, 64, 64], float32)]))]"
     ]
    }
   ],
   "source": [
    "out = subgraphs[0]\n",
    "ann = run_opt_pass(out, transform.ToGraphNormalForm())\n",
    "out = tvm.IRModule.from_expr(ann)['main']\n",
    "# outnew = [run_opt_pass(i, transform.ToGraphNormalForm()) for i in outputs[0]]\n",
    "out1 = relay.Function(out.params, relay.Tuple(outnew), out.ret_type, out.type_params, out.attrs)\n",
    "mod = tvm.IRModule()\n",
    "mod[\"main\"] = out1\n",
    "\n",
    "# with tvm.transform.PassContext(opt_level=4):\n",
    "#     lib = relay.build(out1, 'cuda', params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[LetNode(Var(x_225, ty=TensorType([1, 32, 128, 128], float32)), CallNode(Op(nn.bias_add), [Var(x_223, ty=TensorType([1, 32, 128, 128], float32)), Var(x_224, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x30b172c8), [TensorType([1, 32, 128, 128], float32), TensorType([32], float32)]), LetNode(Var(x_226, ty=TensorType([1, 32, (int64)64, (int64)64], float32)), CallNode(Op(nn.max_pool2d), [Var(x_225, ty=TensorType([1, 32, 128, 128], float32))], relay.attrs.MaxPool2DAttrs(0x30aedf08), [TensorType([1, 32, 128, 128], float32)]), LetNode(Var(x_227, ty=TensorType([64, 32, 3, 3], float32)), Constant([[[[-0.00034386 -0.00349673  0.07773861]\n",
       "     [-0.03190579 -0.01584832  0.02670189]\n",
       "     [ 0.01921258 -0.07818282  0.03479443]]\n",
       "  \n",
       "    [[-0.00130421 -0.01830789  0.05642799]\n",
       "     [-0.0170652  -0.02210277  0.04254849]\n",
       "     [ 0.01875039 -0.08010064 -0.08040661]]\n",
       "  \n",
       "    [[ 0.02165145  0.0240651  -0.04487719]\n",
       "     [-0.02167934 -0.00768209  0.0794398 ]\n",
       "     [-0.02068903  0.06871933  0.07726983]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[-0.03205558  0.00290275 -0.02926568]\n",
       "     [ 0.04182967  0.04922665 -0.06711974]\n",
       "     [-0.02741003  0.05309341  0.01915387]]\n",
       "  \n",
       "    [[-0.04150633  0.08178725  0.03009626]\n",
       "     [ 0.05355413  0.03888881 -0.05639726]\n",
       "     [ 0.04942527 -0.00437707  0.00632731]]\n",
       "  \n",
       "    [[-0.02561722  0.05984905 -0.05160853]\n",
       "     [-0.07984905 -0.05768615  0.07380161]\n",
       "     [-0.07858952 -0.07737446  0.01850816]]]\n",
       "  \n",
       "  \n",
       "   [[[-0.03143118  0.0695676  -0.04244177]\n",
       "     [ 0.04627309 -0.03359256 -0.06305707]\n",
       "     [ 0.01931474  0.06957508 -0.04066815]]\n",
       "  \n",
       "    [[-0.05282148  0.07525725 -0.06407903]\n",
       "     [ 0.05680341  0.03004328 -0.07779044]\n",
       "     [-0.04317856 -0.04223323  0.04984415]]\n",
       "  \n",
       "    [[-0.05401897 -0.03738686 -0.04842353]\n",
       "     [ 0.02915064 -0.01600464  0.07291468]\n",
       "     [-0.03190204 -0.0579984   0.04465697]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[-0.00600344 -0.01771474 -0.0669682 ]\n",
       "     [ 0.00072628 -0.05233977 -0.03741848]\n",
       "     [ 0.07280735 -0.03233246  0.01515931]]\n",
       "  \n",
       "    [[-0.05387771 -0.07664208  0.03958815]\n",
       "     [-0.06479758 -0.02311367 -0.05349527]\n",
       "     [-0.03774371 -0.00293905 -0.01947474]]\n",
       "  \n",
       "    [[-0.06706224 -0.0475895  -0.03024844]\n",
       "     [-0.06104823 -0.05882414  0.03911529]\n",
       "     [-0.03023565  0.0054945   0.03267833]]]\n",
       "  \n",
       "  \n",
       "   [[[-0.05496893  0.04519042 -0.04979972]\n",
       "     [-0.0025231  -0.04394376 -0.0357133 ]\n",
       "     [ 0.06344249  0.04820261 -0.01685154]]\n",
       "  \n",
       "    [[-0.07507978 -0.0369214   0.01589892]\n",
       "     [ 0.07496265 -0.03811238  0.07452969]\n",
       "     [-0.05802975  0.06737594 -0.02538034]]\n",
       "  \n",
       "    [[-0.06388503 -0.06834688 -0.02203981]\n",
       "     [ 0.00349629 -0.03683617 -0.07054752]\n",
       "     [ 0.00112883 -0.06129277 -0.03130053]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[ 0.03836095  0.0297214   0.07942633]\n",
       "     [-0.01441041  0.00916564  0.06147317]\n",
       "     [ 0.08268154  0.01616826  0.08289457]]\n",
       "  \n",
       "    [[ 0.03203704  0.01688347 -0.03459275]\n",
       "     [ 0.00918504 -0.00689121 -0.04055142]\n",
       "     [-0.08139901  0.06464074  0.02731381]]\n",
       "  \n",
       "    [[-0.0072248  -0.07411295 -0.05513853]\n",
       "     [-0.02443562  0.03619969 -0.01066486]\n",
       "     [-0.0525009  -0.04152107 -0.04262205]]]\n",
       "  \n",
       "  \n",
       "   ...\n",
       "  \n",
       "  \n",
       "   [[[-0.06229639  0.00368265 -0.0067502 ]\n",
       "     [-0.03333757  0.02255654  0.01452807]\n",
       "     [ 0.07274816  0.04599313 -0.00011758]]\n",
       "  \n",
       "    [[ 0.07283399  0.05451591 -0.04420283]\n",
       "     [ 0.07164187  0.08015741  0.06468589]\n",
       "     [-0.03062155 -0.05015038  0.02159031]]\n",
       "  \n",
       "    [[ 0.00773998 -0.02750689 -0.03792113]\n",
       "     [-0.00470024  0.00760442 -0.00532582]\n",
       "     [-0.02140979  0.03761176  0.07121743]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[ 0.08014026  0.00458467  0.0824988 ]\n",
       "     [ 0.08175186 -0.01663043  0.01697671]\n",
       "     [-0.04998031 -0.00914244  0.02868303]]\n",
       "  \n",
       "    [[-0.00251392 -0.02931277 -0.01294055]\n",
       "     [ 0.07991172 -0.0130088  -0.05366091]\n",
       "     [ 0.0464421  -0.06485568  0.0763394 ]]\n",
       "  \n",
       "    [[-0.004662   -0.04977087  0.04939119]\n",
       "     [-0.01845763 -0.06482323  0.02449331]\n",
       "     [-0.01165619  0.06523957  0.03770921]]]\n",
       "  \n",
       "  \n",
       "   [[[ 0.01544333 -0.01124992 -0.05245157]\n",
       "     [-0.05209206  0.03204753  0.02439884]\n",
       "     [-0.0282647   0.03098512  0.07474277]]\n",
       "  \n",
       "    [[-0.05827966  0.00631046 -0.04896474]\n",
       "     [ 0.00211457  0.03935728  0.01188233]\n",
       "     [ 0.0395141  -0.0487875  -0.05219557]]\n",
       "  \n",
       "    [[ 0.01199625  0.01175252 -0.08195679]\n",
       "     [-0.04492062 -0.07222724  0.08265557]\n",
       "     [ 0.00680294 -0.06261617 -0.04109164]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[-0.02842131  0.04017299 -0.06411938]\n",
       "     [-0.03479248 -0.00351638  0.04439355]\n",
       "     [-0.00836619 -0.00725466 -0.07700257]]\n",
       "  \n",
       "    [[ 0.00681683  0.06109884 -0.03970103]\n",
       "     [-0.07532208  0.02519193  0.03664758]\n",
       "     [ 0.02089242  0.03030465 -0.04797888]]\n",
       "  \n",
       "    [[ 0.06098434 -0.05125368 -0.07491653]\n",
       "     [-0.00750206 -0.06763705  0.05190892]\n",
       "     [ 0.02197877  0.07384106  0.04836109]]]\n",
       "  \n",
       "  \n",
       "   [[[-0.03292805  0.05048368  0.05959893]\n",
       "     [-0.00935958  0.07049897  0.07671262]\n",
       "     [-0.0200305   0.02493731 -0.05899358]]\n",
       "  \n",
       "    [[-0.06843758  0.01676033  0.07503303]\n",
       "     [-0.00159057  0.07050664 -0.06662065]\n",
       "     [-0.07063043 -0.00717596 -0.07401031]]\n",
       "  \n",
       "    [[-0.05164689  0.01307825  0.0358992 ]\n",
       "     [ 0.07245863  0.0667781   0.00989082]\n",
       "     [ 0.02502123 -0.00115818  0.06598707]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[ 0.00485136 -0.08295983 -0.03392466]\n",
       "     [ 0.06617681  0.03855822  0.08260872]\n",
       "     [ 0.0201846   0.02270871  0.0820188 ]]\n",
       "  \n",
       "    [[-0.04204714  0.02072773 -0.0725994 ]\n",
       "     [ 0.05576969 -0.03805411 -0.05533451]\n",
       "     [ 0.08203562 -0.04242806 -0.03506017]]\n",
       "  \n",
       "    [[ 0.03055421 -0.0600045   0.01011878]\n",
       "     [ 0.07702617  0.05981912  0.04538635]\n",
       "     [-0.0253101  -0.06463408 -0.07805812]]]]), LetNode(Var(x_228, ty=TensorType([1, 64, 64, 64], float32)), CallNode(Op(nn.conv2d), [Var(x_226, ty=TensorType([1, 32, (int64)64, (int64)64], float32)), Var(x_227, ty=TensorType([64, 32, 3, 3], float32))], relay.attrs.Conv2DAttrs(0x306a8508), [TensorType([1, 32, (int64)64, (int64)64], float32), TensorType([64, 32, 3, 3], float32)]), LetNode(Var(x_229, ty=TensorType([64], float32)), Constant([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]), LetNode(Var(x_230, ty=TensorType([1, 64, 64, 64], float32)), CallNode(Op(nn.bias_add), [Var(x_228, ty=TensorType([1, 64, 64, 64], float32)), Var(x_229, ty=TensorType([64], float32))], relay.attrs.BiasAddAttrs(0x30aedb18), [TensorType([1, 64, 64, 64], float32), TensorType([64], float32)]), LetNode(Var(x_231, ty=TensorType([1, 64, (int64)32, (int64)32], float32)), CallNode(Op(nn.max_pool2d), [Var(x_230, ty=TensorType([1, 64, 64, 64], float32))], relay.attrs.MaxPool2DAttrs(0x30b03d38), [TensorType([1, 64, 64, 64], float32)]), LetNode(Var(x_232, ty=TensorType([64, 32, 3, 3], float32)), Constant([[[[-0.00667036  0.07674357  0.02048975]\n",
       "     [-0.0005337   0.06994274 -0.0409746 ]\n",
       "     [-0.01644263  0.02424868 -0.08242689]]\n",
       "  \n",
       "    [[-0.0279611   0.02216059 -0.04442815]\n",
       "     [-0.00076614 -0.00510069  0.00525645]\n",
       "     [ 0.05821113  0.01974467  0.01869611]]\n",
       "  \n",
       "    [[-0.01291529 -0.04766705 -0.04010829]\n",
       "     [-0.06574585 -0.06930234 -0.07820863]\n",
       "     [ 0.05632005 -0.01981469  0.03960329]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[-0.00203542 -0.01508743 -0.05518967]\n",
       "     [ 0.0098869  -0.02211016 -0.05686476]\n",
       "     [ 0.04343555  0.04212476 -0.04262485]]\n",
       "  \n",
       "    [[ 0.04512688 -0.01977579 -0.03921237]\n",
       "     [ 0.05846582 -0.01097473 -0.08292767]\n",
       "     [ 0.02642258 -0.05865683  0.00781246]]\n",
       "  \n",
       "    [[ 0.00868873  0.01789419 -0.05306506]\n",
       "     [ 0.03495514  0.00849197 -0.03360848]\n",
       "     [ 0.05213998 -0.05043467  0.02092493]]]\n",
       "  \n",
       "  \n",
       "   [[[-0.05225816 -0.03435542  0.06766681]\n",
       "     [-0.00570095 -0.07121497  0.04150715]\n",
       "     [ 0.03178158  0.0278095  -0.03169968]]\n",
       "  \n",
       "    [[ 0.08237877  0.00820927  0.07866023]\n",
       "     [ 0.06363843  0.00969597 -0.07367277]\n",
       "     [-0.07699051 -0.07091855 -0.04057622]]\n",
       "  \n",
       "    [[ 0.03507438  0.06507566 -0.07537719]\n",
       "     [-0.02447435 -0.06244735 -0.06095092]\n",
       "     [ 0.01566204  0.02181033  0.05003274]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[ 0.0431037   0.0482654   0.08321209]\n",
       "     [-0.05071964 -0.05884733 -0.0633956 ]\n",
       "     [ 0.06844861 -0.05180594 -0.0485846 ]]\n",
       "  \n",
       "    [[-0.03323257  0.02323034  0.00272747]\n",
       "     [ 0.05667796 -0.0002616  -0.00519013]\n",
       "     [ 0.01162627  0.02526001  0.06600187]]\n",
       "  \n",
       "    [[ 0.07655057  0.02485903 -0.05985588]\n",
       "     [ 0.04408409 -0.0380648  -0.00562543]\n",
       "     [-0.03243604 -0.03266114  0.02271339]]]\n",
       "  \n",
       "  \n",
       "   [[[ 0.02307111 -0.07447599  0.00114187]\n",
       "     [ 0.0209925  -0.03011368 -0.04503453]\n",
       "     [-0.01692474  0.06745539 -0.02362834]]\n",
       "  \n",
       "    [[ 0.06450888 -0.01736359 -0.02165564]\n",
       "     [-0.0565699   0.01656731  0.02207325]\n",
       "     [-0.03282728 -0.07110369 -0.05474281]]\n",
       "  \n",
       "    [[-0.00164378  0.07526512  0.07337628]\n",
       "     [ 0.01917853 -0.02339783  0.00305605]\n",
       "     [-0.05832756 -0.04507156 -0.02815646]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[-0.06093149 -0.06571921 -0.0378267 ]\n",
       "     [-0.05794454  0.02385507  0.04264715]\n",
       "     [ 0.0108671   0.05971035 -0.07129908]]\n",
       "  \n",
       "    [[-0.08070076  0.06473055 -0.0824315 ]\n",
       "     [ 0.02900531 -0.03235545 -0.03446659]\n",
       "     [-0.0361414  -0.0720194  -0.06739503]]\n",
       "  \n",
       "    [[ 0.06519654  0.05091707 -0.03405452]\n",
       "     [ 0.01338766  0.07065854 -0.01776394]\n",
       "     [ 0.03464925  0.03024761  0.03585237]]]\n",
       "  \n",
       "  \n",
       "   ...\n",
       "  \n",
       "  \n",
       "   [[[ 0.03777675 -0.03727138  0.01838966]\n",
       "     [-0.08274333 -0.064996    0.07656416]\n",
       "     [ 0.00311184  0.00922922  0.08024732]]\n",
       "  \n",
       "    [[-0.04885089  0.00447249  0.076678  ]\n",
       "     [-0.06130725 -0.08112308  0.01622806]\n",
       "     [ 0.00925171  0.07307453  0.00239494]]\n",
       "  \n",
       "    [[ 0.06266668 -0.07882553 -0.02372478]\n",
       "     [-0.00127685 -0.05542324  0.01506688]\n",
       "     [-0.04899919 -0.02099749 -0.04583566]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[-0.01963693 -0.01299276 -0.00876871]\n",
       "     [ 0.03455848  0.01813295 -0.01817822]\n",
       "     [ 0.00511507  0.01796635  0.03721446]]\n",
       "  \n",
       "    [[ 0.02849569 -0.0781398   0.06691042]\n",
       "     [ 0.08145543 -0.06311989 -0.06119695]\n",
       "     [-0.08139144 -0.04604743 -0.03518212]]\n",
       "  \n",
       "    [[-0.04671299  0.06773625 -0.05673073]\n",
       "     [-0.06609975  0.01549089  0.02074935]\n",
       "     [ 0.07406517 -0.04903018 -0.04878726]]]\n",
       "  \n",
       "  \n",
       "   [[[-0.02131156  0.06878123  0.02314601]\n",
       "     [ 0.07108916  0.04008206  0.02196169]\n",
       "     [-0.07365716 -0.03871602  0.04667952]]\n",
       "  \n",
       "    [[ 0.02348771  0.02149502  0.02541289]\n",
       "     [-0.01793929  0.06772844  0.02993079]\n",
       "     [-0.07476664  0.01974591 -0.07887741]]\n",
       "  \n",
       "    [[ 0.05220968  0.00943748  0.03035134]\n",
       "     [ 0.03464494 -0.02933544 -0.04283194]\n",
       "     [ 0.06609956  0.06831052 -0.05975342]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[ 0.00016204  0.0783995   0.07785139]\n",
       "     [ 0.03268198 -0.01940602  0.05926634]\n",
       "     [-0.05186518 -0.06860392  0.07977588]]\n",
       "  \n",
       "    [[ 0.06457258  0.077715   -0.05051591]\n",
       "     [-0.07378592 -0.07513899 -0.02788895]\n",
       "     [-0.06781612 -0.02720525  0.08009199]]\n",
       "  \n",
       "    [[-0.00609747 -0.00378752  0.06699146]\n",
       "     [-0.00971287 -0.05465307 -0.02018831]\n",
       "     [ 0.08092386 -0.01566859 -0.06308937]]]\n",
       "  \n",
       "  \n",
       "   [[[-0.01809273 -0.05052249  0.00830408]\n",
       "     [ 0.00651675  0.02668866 -0.02696224]\n",
       "     [ 0.06081321  0.03202073 -0.04717416]]\n",
       "  \n",
       "    [[ 0.0587954   0.06785775  0.06052122]\n",
       "     [ 0.00263701 -0.01710707 -0.00749189]\n",
       "     [ 0.02028066 -0.00381591  0.05403914]]\n",
       "  \n",
       "    [[-0.06002963  0.00170328  0.01363794]\n",
       "     [ 0.02595466  0.02344725 -0.03352585]\n",
       "     [ 0.06551506  0.0296605   0.06894781]]\n",
       "  \n",
       "    ...\n",
       "  \n",
       "    [[ 0.04791441  0.06846394  0.05488243]\n",
       "     [-0.04654666 -0.07713521  0.08156196]\n",
       "     [ 0.03400215  0.06116603 -0.02428303]]\n",
       "  \n",
       "    [[ 0.06481076  0.04652277 -0.04640434]\n",
       "     [-0.02026961  0.07876632  0.04329368]\n",
       "     [-0.06511961 -0.05031322 -0.04249084]]\n",
       "  \n",
       "    [[-0.05632734 -0.02485887 -0.06702858]\n",
       "     [-0.0249401  -0.0442156   0.00897348]\n",
       "     [ 0.06168193  0.00339633  0.07160915]]]]), LetNode(Var(x_233, ty=TensorType([1, 32, 64, 64], float32)), CallNode(Op(nn.conv2d_transpose), [Var(x_231, ty=TensorType([1, 64, (int64)32, (int64)32], float32)), Var(x_232, ty=TensorType([64, 32, 3, 3], float32))], relay.attrs.Conv2DTransposeAttrs(0x33f4a08), [TensorType([1, 64, (int64)32, (int64)32], float32), TensorType([64, 32, 3, 3], float32)]), LetNode(Var(x_234, ty=TensorType([32], float32)), Constant([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       "   0. 0. 0. 0. 0. 0. 0. 0.]), LetNode(Var(x_235, ty=TensorType([1, 32, 64, 64], float32)), CallNode(Op(nn.bias_add), [Var(x_233, ty=TensorType([1, 32, 64, 64], float32)), Var(x_234, ty=TensorType([32], float32))], relay.attrs.BiasAddAttrs(0x56e1288), [TensorType([1, 32, 64, 64], float32), TensorType([32], float32)]), LetNode(Var(x_236, ty=TupleTypeNode([TensorType([1, 32, 64, 64], float32), TensorType([1, 64, 64, 64], float32)])), Tuple([Var(x_235, ty=TensorType([1, 32, 64, 64], float32)), Var(x_230, ty=TensorType([1, 64, 64, 64], float32))]), LetNode(Var(x_237, ty=TensorType([1, 96, 64, 64], float32)), CallNode(Op(concatenate), [Var(x_236, ty=TupleTypeNode([TensorType([1, 32, 64, 64], float32), TensorType([1, 64, 64, 64], float32)]))], relay.attrs.ConcatenateAttrs(0x56e1318), [TupleTypeNode([TensorType([1, 32, 64, 64], float32), TensorType([1, 64, 64, 64], float32)])]), Var(x_237, ty=TensorType([1, 96, 64, 64], float32))))))))))))))),\n",
       "  LetNode(Var(x_237, ty=TensorType([1, 96, 64, 64], float32)), CallNode(Op(concatenate), [Var(x_236, ty=TupleTypeNode([TensorType([1, 32, 64, 64], float32), TensorType([1, 64, 64, 64], float32)]))], relay.attrs.ConcatenateAttrs(0x56e1318), [TupleTypeNode([TensorType([1, 32, 64, 64], float32), TensorType([1, 64, 64, 64], float32)])]), Var(x_237, ty=TensorType([1, 96, 64, 64], float32)))],\n",
       " []]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ann = run_opt_pass(subgraphs.body, transform.ToGraphNormalForm())\n",
    "tmp_mod = tvm.IRModule.from_expr(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x_20'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod['main'].params[0].name_hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = quantize(mod, 2)\n",
    "dev = tvm.cuda(0)\n",
    "target = 'cuda'\n",
    "# with tvm.transform.PassContext(opt_level=4, disabled_pass={}):\n",
    "#     lib = relay.build(out, target, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = tvm.IRModule()\n",
    "mod['main'] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_attr = {\"color\": \"red\"}\n",
    "node_attr = {\"color\": \"blue\"}\n",
    "edge_attr = {\"color\": \"black\"}\n",
    "def get_node_attr(node):\n",
    "    if \"nn.conv2d\" in node.type_name and \"NCHW\" in node.detail:\n",
    "        return {\n",
    "            \"fillcolor\": \"green\",\n",
    "            \"style\": \"filled\",\n",
    "            \"shape\": \"box\",\n",
    "        }\n",
    "    if \"Var\" in node.type_name:\n",
    "        return {\"shape\": \"ellipse\"}\n",
    "    return {\"shape\": \"box\"}\n",
    "    \n",
    "dot_plotter = relay_viz.DotPlotter(\n",
    "    graph_attr=graph_attr,\n",
    "    node_attr=node_attr,\n",
    "    edge_attr=edge_attr,\n",
    "    get_node_attr=get_node_attr)\n",
    "\n",
    "viz = relay_viz.RelayVisualizer(\n",
    "    # submods[1],\n",
    "    mod,\n",
    "    relay_param=params,\n",
    "    plotter=dot_plotter,\n",
    "    parser=relay_viz.DotVizParser())\n",
    "viz.render(\"newnew\")\n",
    "\n",
    "# viz = relay_viz.RelayVisualizer(\n",
    "#     # submods[0],\n",
    "#     out1,\n",
    "#     relay_param=params,\n",
    "#     plotter=dot_plotter,\n",
    "#     parser=relay_viz.DotVizParser())\n",
    "# viz.render(\"simple_0\")\n",
    "# viz = relay_viz.RelayVisualizer(\n",
    "#     mod,\n",
    "#     relay_param=params,\n",
    "#     plotter=dot_plotter,\n",
    "#     parser=relay_viz.DotVizParser())\n",
    "# viz.render(\"simple_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[version = \"0.0.5\"]\n",
      "free_var %input_1: Tensor[(1, 3, 256, 256), float32] /* ty=Tensor[(1, 3, 256, 256), float32] */;\n",
      "%0 = nn.conv2d(%input_1, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n",
      "%1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float32] */) /* ty=Tensor[(1, 16, 256, 256), float32] */;\n",
      "%2 = nn.max_pool2d(%1, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 16, 128i64, 128i64), float32] */;\n",
      "%3 = nn.conv2d(%2, meta[relay.Constant][2] /* ty=Tensor[(32, 16, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=32, kernel_size=[3, 3]) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n",
      "%4 = nn.bias_add(%3, meta[relay.Constant][3] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 128, 128), float32] */;\n",
      "%5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 32, 64i64, 64i64), float32] */;\n",
      "%6 = nn.conv2d(%5, meta[relay.Constant][4] /* ty=Tensor[(64, 32, 3, 3), float32] */, padding=[1i64, 1i64, 1i64, 1i64], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n",
      "%7 = nn.bias_add(%6, meta[relay.Constant][5] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(1, 64, 64, 64), float32] */;\n",
      "%8 = nn.max_pool2d(%7, pool_size=[2, 2], strides=[2, 2], padding=[0i64, 0i64, 0i64, 0i64]) /* ty=Tensor[(1, 64, 32i64, 32i64), float32] */;\n",
      "%9 = nn.conv2d_transpose(%8, meta[relay.Constant][6] /* ty=Tensor[(64, 32, 3, 3), float32] */, channels=32, kernel_size=[3, 3], strides=[2, 2], padding=[0i64, 0i64, 1i64, 1i64], kernel_layout=\"IOHW\") /* ty=Tensor[(1, 32, 64, 64), float32] */;\n",
      "%10 = nn.bias_add(%9, meta[relay.Constant][7] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(1, 32, 64, 64), float32] */;\n",
      "%11 = (%10, %7) /* ty=(Tensor[(1, 32, 64, 64), float32], Tensor[(1, 64, 64, 64), float32]) */;\n",
      "%12 = concatenate(%11, axis=1) /* ty=Tensor[(1, 96, 64, 64), float32] */;\n",
      "(%4, %12) /* ty=(Tensor[(1, 32, 128, 128), float32], Tensor[(1, 96, 64, 64), float32]) */\n",
      "/* For debugging purposes the metadata section has been omitted.\n",
      " * If you would like to see the full metadata section you can set the \n",
      " * option to `True` when invoking `astext`. \n",
      " */\n"
     ]
    }
   ],
   "source": [
    "print(mod['main'].body.astext(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network():\n",
    "    img_size = 8\n",
    "    out_channels = 16\n",
    "    batch_size = 1\n",
    "    data = relay.var(\"data\", relay.TensorType((batch_size, 3, img_size, img_size), \"float16\"))\n",
    "    dense_weight = relay.var(\n",
    "        \"dweight\", relay.TensorType((batch_size, 16 * img_size * img_size), \"float16\")\n",
    "    )\n",
    "    weight = relay.var(\"weight\")\n",
    "    second_weight = relay.var(\"second_weight\")\n",
    "    bn_gamma = relay.var(\"bn_gamma\")\n",
    "    bn_beta = relay.var(\"bn_beta\")\n",
    "    bn_mmean = relay.var(\"bn_mean\")\n",
    "    bn_mvar = relay.var(\"bn_var\")\n",
    "    simple_net = relay.nn.conv2d(\n",
    "        data=data, weight=weight, kernel_size=(3, 3), channels=out_channels, padding=(1, 1)\n",
    "    )\n",
    "    simple_net = relay.nn.batch_norm(simple_net, bn_gamma, bn_beta, bn_mmean, bn_mvar)[0]\n",
    "    simple_net = relay.nn.relu(simple_net)\n",
    "    simple_net = relay.nn.batch_flatten(simple_net)\n",
    "    simple_net = relay.nn.dense(simple_net, dense_weight)\n",
    "    simple_net = relay.Function(relay.analysis.free_vars(simple_net), simple_net)\n",
    "    data_shape = (batch_size, 3, img_size, img_size)\n",
    "    net, params = testing.create_workload(simple_net)\n",
    "    return net, params, data_shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygraphviz as pgv\n",
    "import json\n",
    "def show_graph(json_data, file_name=None):\n",
    "    if type(json_data) == str:\n",
    "        json_data = json.loads(json_data)\n",
    "    A = pgv.AGraph(directed=True)\n",
    "    for node_idx, node in enumerate(json_data['nodes']):\n",
    "        for src in node['inputs']:\n",
    "            # if args.show_size == 1:\n",
    "            if 1 == 1:\n",
    "                src_size = 1\n",
    "                for i in json_data['attrs']['shape'][1][src[0]]:\n",
    "                    src_size = src_size * i\n",
    "                \n",
    "                dst_size = 1\n",
    "                for i in json_data['attrs']['shape'][1][node_idx]:\n",
    "                    dst_size = dst_size * i\n",
    "                # print(src[0], json_data['nodes'][src[0]]['name'], src_size)\n",
    "\n",
    "                A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]) + \"[{}]\".format(src_size), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]) + \"[{}]\".format(dst_size))\n",
    "            else:\n",
    "                A.add_edge(json_data['nodes'][src[0]]['name'] + '[{}]'.format(src[0]) + '{}'.format(json_data['attrs']['dltype'][1][src[0]]), node['name'] + '[{}]'.format(node_idx) + '{}'.format(json_data['attrs']['dltype'][1][node_idx]))\n",
    "    if file_name:\n",
    "        A.draw(file_name + '.png', format='png', prog='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(graph_json_raw, 'test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
